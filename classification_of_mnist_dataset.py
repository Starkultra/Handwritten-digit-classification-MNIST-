# -*- coding: utf-8 -*-
"""Classification of MNIST Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_d5ldnVmayfjFZCIdmo4QOnj1NebhAOm

Logistic Regression Using Pytorch
"""

import torchvision
from torchvision.datasets import MNIST

dataset=MNIST(root='/content/',download=True)

len(dataset)

test=MNIST(root='/content/',train=False)

len(test)

dataset[0]

# Commented out IPython magic to ensure Python compatibility.
import torch
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

image,label=dataset[0]
plt.imshow(image,cmap='gray')
print("Label : ",label)

image,label=dataset[10]
plt.imshow(image,cmap='gray')
print("Label : ",label)

import torchvision.transforms as transforms

dataset=MNIST(root='/content/',train=True,transform=transforms.ToTensor())

img_tensor,label=dataset[0]
print(img_tensor.shape,label)
#1-no. of channels
#28,28 indicates pixels(height and length)

print(img_tensor[:,10:15,10:15])
print(torch.max(img_tensor),torch.min(img_tensor))

plt.imshow(img_tensor[0,10:15,10:15],cmap='gray');

from torch.utils.data import random_split
from torch.utils.data.dataloader import DataLoader
train_ds,val_ds=random_split(dataset,[50000,10000])

batch_size=100
train_loader=DataLoader(train_ds,batch_size,shuffle=True)
val_loader=DataLoader(val_ds,batch_size)

"""Model Building"""

import torch.nn as nn
import torch.nn.functional as F

input_size=28*28
output_size=10
model=nn.Linear(input_size,output_size)

print(model.weight.shape)

model.weight

print(model.bias.shape)
model.bias

class MnistModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(input_size, output_size)
        
    def forward(self, xb):
        xb = xb.reshape(-1, 784)
        out = self.linear(xb)
        return out
    
model = MnistModel()

print(model.linear.weight.shape,model.linear.bias.shape)

list(model.parameters())

for images, labels in train_loader:
    outputs = model(images)
    break

print('outputs.shape : ', outputs.shape)
print('Sample outputs :\n', outputs[:2].data)

probs = F.softmax(outputs,dim=1)
print("Sample Probabilities : ",probs[:2].data)
print("Sum : ",torch.sum(probs[0]).item())

max_probs,preds=torch.max(probs,dim=1)
print(max_probs)
print(preds)

print(labels)

"""Evalutation Metrics And Loss Function"""

def accuracy(outputs,labels):
  _,preds=torch.max(outputs,dim=1)
  return torch.tensor(torch.sum(preds==labels).item()/len(preds))

accuracy(outputs,labels)

loss_fn=F.cross_entropy
loss=loss_fn(outputs,labels)

print(loss)

"""Training Model"""

class MnistModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear=nn.Linear(input_size,output_size)

  def forward(self,xb):
    xb=xb.reshape(-1,784)
    out=self.linear(xb)
    return out

  def training_step(self,batch):
    images,labels=batch
    out=self(images)
    loss=F.cross_entropy(out,labels)
    return loss

  def validation_step(self,batch):
    images,labels=batch
    out=self(images)
    val_loss=F.cross_entropy(out,labels)
    val_acc=accuracy(out,labels)
    return {"val_loss": val_loss,"val_acc":val_acc}

  def validation_epoch_end(self, outputs):
    batch_losses = [x['val_loss'] for x in outputs]
    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
    batch_accs = [x['val_acc'] for x in outputs]
    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

  def epoch_end(self, epoch, result):
    if (epoch+1) % 10 == 0 :
      print("Epoch [{}],train_loss:{:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(epoch+1,result['train_loss'], result['val_loss'], result['val_acc']))


model=MnistModel()

def evaluate(model, val_loader):
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):
        # Training Phase 
        train_loss=[]
        for batch in train_loader:
            loss = model.training_step(batch)
            train_loss.append(loss)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        # Validation phase
        result = evaluate(model, val_loader)
        result['train_loss'] = torch.stack(train_loss).mean().item()
        model.epoch_end(epoch, result)
        history.append(result)
    return history

# 1st iteration
result=evaluate(model,val_loader)
result

history=fit(50,0.001,model,train_loader,val_loader)

history = [result] + history
accuracies = [result['val_acc'] for result in history]
plt.plot(accuracies, '-x')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.title('Accuracy vs. No. of epochs');

def plot_losses(history):
    train_losses = [x.get('train_loss') for x in history]
    val_losses = [x['val_loss'] for x in history]
    plt.plot(train_losses, '-bx')
    plt.plot(val_losses, '-rx')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['Training', 'Validation'])
    plt.title('Loss vs. No. of epochs');
plot_losses(history)

"""Testing With individuals images"""

images,labels=test[0]
plt.imshow(images,cmap='gray')
print("Labels :",labels)

test=MNIST(root='/content/',train=False,transform=transforms.ToTensor())

img_tensor,labels=test[0]
print("Image shape",img_tensor.shape)
print("Label : ",labels)

img_tensor.unsqueeze(0).shape

def predict_image(img_tensor, model):
    xb = img.unsqueeze(0)
    yb = model(xb)
    _, preds  = torch.max(yb, dim=1)
    return preds[0].item()

img, label = test[0]
plt.imshow(img[0], cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

img, label = test[10]
plt.imshow(img[0], cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

img, label = test[193]
plt.imshow(img[0], cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

img, label = test[1839]
plt.imshow(img[0], cmap='gray')
print('Label:', label, ', Predicted:', predict_image(img, model))

test_loader = DataLoader(test, batch_size=256)
result = evaluate(model, test_loader)
result

torch.save(model.state_dict(), 'mnist-logistic.pth')

model.state_dict()

model2 = MnistModel()
model2.load_state_dict(torch.load('mnist-logistic.pth'))
model2.state_dict()

# Retaining the save .pth file and cross checking 
test_loader = DataLoader(test, batch_size=256)
result = evaluate(model2, test_loader)
result



"""By logistic Regression we got accuarcy of 88% if num_epoch increases will leads to achieve 90 % may be!! ;)

Increasing Neural Networks for improving accuracy ( Deep Neural Network)
"""

dataset

images,labels=dataset[0]
plt.imshow(images[0],cmap='gray')
print("Label : ",labels)

def split_indices(n,val_pct):
  n_val=int(n*val_pct)
  idx=np.random.permutation(n)
  return idx[n_val:],idx[:n_val]

train_idx,val_idx=split_indices(len(dataset),val_pct=0.2)
print(len(train_idx),len(val_idx))
print("Sample Indices : ",val_idx[:20])

images,labels=dataset[248]
plt.imshow(images[0],cmap='gray')
print("Label : ",labels)

from torch.utils.data.sampler import SubsetRandomSampler as sam

batch_size=100
train_sampler=sam(train_idx)
train_loader=DataLoader(dataset,
                        batch_size,
                        sampler=train_sampler)
val_sampler=sam(val_idx)
val_loader=DataLoader(dataset,
                      batch_size,
                      sampler=val_sampler)

"""Training Deep Neural Network"""

from torchvision.utils import make_grid
from torch.utils.data import random_split
from torchvision.transforms import ToTensor

batch_size=128
train_loader=DataLoader(train_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)
val_loader=DataLoader(val_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)

for images,_ in train_loader:
  print("Images.Shape : ",images.shape)
  plt.figure(figsize=(16,8))
  plt.axis("off")
  plt.imshow(make_grid(images,nrow=16).permute((1,2,0)))
  break

def accuracy(outputs,labels):
  _,preds=torch.max(outputs,dim=1)
  return torch.tensor(torch.sum(preds==labels).item()/len(preds))

class MnistModel(nn.Module):
    """Feedfoward neural network with 1 hidden layer"""
    def __init__(self, in_size, hidden_size, out_size):
        super().__init__()
        self.linear1 = nn.Linear(in_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)
        self.linear3=nn.Linear(hidden_size,hidden_size)
        self.linear4=nn.Linear(hidden_size,out_size)
        
    def forward(self, xb):
        xb = xb.reshape(-1,784)
        out = self.linear1(xb)
        out = F.relu(out)
        out = self.linear2(out)
        out =  F.relu(out)
        out = self.linear3(out)
        out = F.relu(out)
        out = self.linear4(out)
        return out
    
    def training_step(self, batch):
        images, labels = batch 
        out = self(images)                  # Generate predictions
        loss = F.cross_entropy(out, labels) # Calculate loss
        return loss
    
    def validation_step(self, batch):
        images, labels = batch 
        out = self(images)                    # Generate predictions
        loss = F.cross_entropy(out, labels)   # Calculate loss
        acc = accuracy(out, labels)           # Calculate accuracy
        return {'val_loss': loss, 'val_acc': acc}
        
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
      if(epoch+1)%10==0:
        print("Epoch [{}], val_acc: {:.4f}, val_loss: {:.4f}".format(epoch+1, result['val_acc'], result['val_loss']))

input_size=784
batch_size=32
num_classes=10

model = MnistModel(input_size, hidden_size=32, out_size=num_classes)

for t in model.parameters():
  print(t.shape)

for images, labels in train_loader:
  outputs = model(images)
  loss = F.cross_entropy(outputs, labels)
  print('Loss:', loss.item())
  break

print('outputs.shape : ', outputs.shape)
print('Sample outputs :\n', outputs[:2].data)

torch.cuda.is_available()

def get_default_device():
  if torch.cuda.is_available():
    return torch.device("cuda")
  else:
    return torch.device("cpu")

device=get_default_device()
device

def to_device(data,device):
  if isinstance(data,(list,tuple)):
    return [to_device(x,device) for x in data]
  return data.to(device,non_blocking=True)

for images,labels in train_loader:
  print("Images.Shape :",images.shape)
  images=to_device(images,device)
  print(images.device)
  break

class DeviceDataLoader():
  def __init__(self,dl,device):
    self.dl=dl
    self.device=device

  def __iter__(self):
    for b in self.dl:
      yield to_device(b,self.device)

  def __len__(self):
    return len(self.dl)

train_loader=DeviceDataLoader(train_loader,device)
val_loader=DeviceDataLoader(val_loader,device)

for xb,yb in val_loader:
  print("xb.device : ",xb.device)
  print("yb :",yb)
  break

"""Traning the Model"""

def evaluate(model, val_loader):
  outputs = [model.validation_step(batch) for batch in val_loader]
  return model.validation_epoch_end(outputs)

def fit(epochs,lr,model,train_loader,val_loader,opt_func=torch.optim.SGD):
  history=[]
  optimizer=opt_func(model.parameters(),lr)
  for epoch in range(epochs):
    for batch in train_loader:
      loss=model.training_step(batch)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
    result=evaluate(model, val_loader)
    model.epoch_end(epoch,result)
    history.append(result)
  return history

model=MnistModel(input_size, hidden_size=32, out_size=num_classes)
to_device(model,device)

history1=[evaluate(model,val_loader)]
history1

history1+= fit(50, 0.5, model, train_loader, val_loader)

losses=[x['val_loss'] for x in history]
plt.plot(losses,'-x')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Loss vs #of Epoch')

accuracies=[x['val_acc'] for x in history]
plt.plot(accuracies,'-x')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.title('accuracy vs #of Epoch')

"""Damnnn!! By increasing #of hidden layer got 95% accuracy
three hidden layers and 1 output layer
"""